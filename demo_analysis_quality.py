#!/usr/bin/env python3
"""
Demo: How Agent-UXR Extracts Meaningful Insights and Quotes

This script demonstrates the difference between generic analysis and 
UX-focused analysis with actual quotes and specific insights.
"""

import os
from dotenv import load_dotenv

load_dotenv()

def show_analysis_comparison():
    """Show the difference between generic vs UX-focused analysis"""
    
    print("üîç HOW AGENT-UXR EXTRACTS MEANINGFUL INSIGHTS")
    print("=" * 60)
    
    print("\nüìÑ SAMPLE USER INTERVIEW EXCERPT:")
    print("-" * 40)
    sample_text = """
    Moderator: What are your first impressions of this dashboard?
    
    Participant: Well, the first thing I notice is that it's quite clean and modern looking. 
    I like the color scheme. However, I'm immediately overwhelmed by the number of widgets 
    on the screen. There's a lot of information competing for my attention.
    
    Moderator: How intuitive does the navigation feel?
    
    Participant: The navigation is a bit confusing to be honest. I see these tabs at the top, 
    but I'm not sure what the difference is between "Analytics" and "Insights" - those feel 
    like they could be the same thing. And this sidebar menu has so many options that I 
    don't know where to start.
    """
    print(sample_text)
    
    print("\n‚ùå GENERIC ANALYSIS (OLD APPROACH):")
    print("-" * 40)
    generic_insights = [
        "Users want better design",
        "Navigation needs improvement", 
        "Users have feedback about the interface",
        "The dashboard could be more user-friendly",
        "Users expressed some concerns"
    ]
    for insight in generic_insights:
        print(f"‚Ä¢ {insight}")
    
    print("\n‚úÖ UX-FOCUSED ANALYSIS (ENHANCED APPROACH):")
    print("-" * 40)
    ux_insights = [
        'üéØ SPECIFIC PAIN POINT: Information hierarchy causes cognitive overload - user said "I\'m immediately overwhelmed by the number of widgets" indicating poor visual prioritization',
        
        'üß† MENTAL MODEL MISMATCH: Navigation taxonomy unclear - user stated "I\'m not sure what the difference is between Analytics and Insights - those feel like they could be the same thing" showing terminology confusion',
        
        '‚ö° WORKFLOW INTERRUPTION: Decision paralysis in sidebar - "this sidebar menu has so many options that I don\'t know where to start" suggests overwhelming choice architecture',
        
        'üí° POSITIVE FEEDBACK: Visual design appreciated - "clean and modern looking. I like the color scheme" shows aesthetic success to maintain',
        
        'üîß ROOT CAUSE: Competing attention without clear hierarchy - "a lot of information competing for my attention" indicates need for visual weight prioritization'
    ]
    for insight in ux_insights:
        print(f"‚Ä¢ {insight}")
    
    print("\nüí¨ EXTRACTED QUOTES FOR EVIDENCE:")
    print("-" * 40)
    quotes = [
        "I'm immediately overwhelmed by the number of widgets on the screen",
        "I'm not sure what the difference is between 'Analytics' and 'Insights'",
        "This sidebar menu has so many options that I don't know where to start",
        "There's a lot of information competing for my attention",
        "It's quite clean and modern looking. I like the color scheme"
    ]
    for i, quote in enumerate(quotes, 1):
        print(f'{i}. "{quote}"')
    
    print("\nüéØ ACTIONABLE RECOMMENDATIONS:")
    print("-" * 40)
    recommendations = [
        "IMMEDIATE: Consolidate 'Analytics' and 'Insights' tabs into single 'Data' section with clear sub-navigation",
        "SHORT-TERM: Implement visual hierarchy with primary/secondary widget styling to reduce cognitive load", 
        "MED-TERM: Add customizable dashboard with role-based defaults to reduce sidebar complexity",
        "LONG-TERM: Conduct card sorting study to redesign information architecture based on user mental models"
    ]
    for i, rec in enumerate(recommendations, 1):
        print(f"{i}. {rec}")

def show_prompt_engineering_strategy():
    """Explain how the prompts are engineered for quality"""
    
    print("\n\nüß† PROMPT ENGINEERING STRATEGY")
    print("=" * 60)
    
    strategies = [
        {
            "technique": "Role-Based Context",
            "description": "AI assumes senior UX researcher perspective",
            "example": "You are a senior UX researcher with expertise in user experience design"
        },
        {
            "technique": "Specific Instructions", 
            "description": "Clear directives for what to extract and avoid",
            "example": "AVOID generic UX boilerplate. Be specific to what users actually said."
        },
        {
            "technique": "Structured Output",
            "description": "JSON format ensures consistent, parseable results",
            "example": "Return JSON with: themes, pain_points, recommendations, user_quotes"
        },
        {
            "technique": "Example-Driven Learning",
            "description": "Shows AI good vs bad analysis examples",
            "example": "Instead of: 'Users want better navigation' Write: 'Users confused by...'"
        },
        {
            "technique": "Evidence Requirement",
            "description": "Demands quotes and specific examples",
            "example": "Include direct quotes that support each insight"
        }
    ]
    
    for strategy in strategies:
        print(f"\nüéØ {strategy['technique']}")
        print(f"   Purpose: {strategy['description']}")
        print(f"   Example: {strategy['example']}")

if __name__ == "__main__":
    show_analysis_comparison()
    show_prompt_engineering_strategy()
    
    print(f"\n\nüöÄ TRY IT YOURSELF:")
    print("-" * 40)
    print("1. Add your OpenAI API key to .env file")
    print("2. Run: python main_enhanced.py sample_data/user_interview_dashboard.txt")
    print("3. Compare the output to see the difference!")
    print("\nüí° The AI now extracts specific, actionable insights with user quotes as evidence.")